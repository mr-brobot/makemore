{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/names.txt') as names_file:\n",
    "    words = names_file.read().splitlines()\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import ascii_lowercase\n",
    "\n",
    "BOUNDARY = \".\"\n",
    "\n",
    "vocab = [BOUNDARY] + list(ascii_lowercase)\n",
    "\n",
    "vtoi = {v: i for i, v in enumerate(vocab)}\n",
    "\n",
    "vtoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 80%, used to train parameters\n",
    "\n",
    "Dev: 10%, used to test hyperparameters\n",
    "\n",
    "Test: 10%, used to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Collection, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "n = 3\n",
    "\n",
    "def build_dataset(words: \"Collection[str]\", n: \"int\" = 3) -> \"Tuple[torch.Tensor, torch.Tensor]\":\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * n\n",
    "        for c in word + BOUNDARY:\n",
    "            i = vtoi[c]\n",
    "            X.append(context)\n",
    "            Y.append(i)\n",
    "\n",
    "            # update context (crop & append)\n",
    "            context = context[1:] + [i]\n",
    "\n",
    "    X = torch.tensor(X).short()\n",
    "    Y = torch.tensor(Y).short()\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "ix_dev = int(len(words) * 0.8)\n",
    "ix_test = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:ix_dev])\n",
    "Xdev, Ydev = build_dataset(words[ix_dev:ix_test])\n",
    "Xtest, Ytest = build_dataset(words[ix_test:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utility function will be used to compare gradients computed by our model with those computed by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cmp(name: \"str\", dt: \"torch.Tensor\", t: \"torch.Tensor\"):\n",
    "    exact = torch.all(dt == t.grad).item()\n",
    "    approx = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f\"{name:15s} | exact: {str(exact):5s} | approx: {str(approx):5s} | maxdiff: {maxdiff:10.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Many of these parameters are being initialized in non-standard ways. Some initialization best practices e.g. all zeros could mask incorrect implementations of backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12097"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "emb_size = 10 # dimensionality of the embedding\n",
    "h_size = 200 # number of neurons in hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn(len(vocab), emb_size, generator=g)\n",
    "\n",
    "# layer 1\n",
    "W1 = torch.randn(n * emb_size, h_size, generator=g) * (5/3) / ((n * emb_size)**0.5)\n",
    "\n",
    "# layer 2\n",
    "W2 = torch.randn(h_size, len(vocab), generator=g) * 0.1 # initialize with small values, creates uniform distribution of initial probabilities\n",
    "b2 = torch.randn(len(vocab), generator=g) * 0.1\n",
    "\n",
    "# batchnorm parameters\n",
    "bn_gain = torch.randn((1, h_size), generator=g) * 0.1 + 1.0\n",
    "bn_bias = torch.randn((1, h_size), generator=g) * 0.1\n",
    "\n",
    "parameters = [C, W1, W2, b2, bn_gain, bn_bias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# total parameters\n",
    "sum(p.numel() for p in parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5586, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb.long()]\n",
    "embcat = emb.view(batch_size, -1)\n",
    "\n",
    "# linear layer 1\n",
    "h_prenorm = embcat @ W1\n",
    "\n",
    "# batchnorm\n",
    "bnmean = 1/batch_size * h_prenorm.sum(0, keepdim=True)\n",
    "bn_diff = h_prenorm - bnmean\n",
    "bn_dff_sq = bn_diff**2\n",
    "bn_var = 1/(batch_size-1) * bn_dff_sq.sum(0, keepdim=True) # note: bessel's correction (dividing by n-1 instead of n)\n",
    "bn_var_inv = (bn_var + 1e-5)**-0.5\n",
    "bn_raw = bn_diff * bn_var_inv\n",
    "h_norm = bn_gain * bn_raw + bn_bias\n",
    "\n",
    "# activation\n",
    "h = torch.tanh(h_norm)\n",
    "\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# cross-entropy loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[range(batch_size), Yb.long()].mean()\n",
    "\n",
    "# pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "tensors = [\n",
    "    emb,\n",
    "    embcat,\n",
    "    h_prenorm,\n",
    "    bnmean,\n",
    "    bn_diff,\n",
    "    bn_dff_sq,\n",
    "    bn_var,\n",
    "    bn_var_inv,\n",
    "    bn_raw,\n",
    "    h_norm,\n",
    "    h,\n",
    "    logits,\n",
    "    logit_maxes,\n",
    "    norm_logits,\n",
    "    counts,\n",
    "    counts_sum,\n",
    "    counts_sum_inv,\n",
    "    probs,\n",
    "    log_probs,\n",
    "    loss,\n",
    "]\n",
    "\n",
    "for t in tensors:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_logprobs = None\n",
    "\n",
    "d_probs = None\n",
    "\n",
    "d_counts_sum_inv = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
